Web Scraping
Beautiful Soup: BeautifulSoup is a Python library used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.

from bs4 import BeautifulSoup
import requests
URL = "http://www.example.com"
page = requests.get(URL)
soup = BeautifulSoup(page.content, "html.parser")

Scrapy: Scrapy is an open-source and collaborative web crawling framework for Python. It is used to extract the data from the website.

import scrapy
class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ['http://quotes.toscrape.com/tag/humor/',]
    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {'quote': quote.css('span.text::text').get()}

Selenium: Selenium is a tool used for controlling web browsers through programs and automating browser tasks.

from selenium import webdriver
driver = webdriver.Firefox()
driver.get("http://www.example.com")

Applications of Web Scraping
Web scraping is used in various fields and has many applications:

Price Comparison: Services such as ParseHub use web scraping to collect data from online shopping websites and use it to compare the prices of products.

Email address gathering: Many companies that use email as a medium for marketing, use web scraping to collect email ID and then send bulk emails.

Social Media Scraping: Web scraping is used to collect data from Social Media websites such as Twitter to find out what's trending.

In this video we will review Hypertext Markup Language or HTML for Webscraping. Lots of useful data is available on web pages, such as real estate prices and solutions to coding questions. The website Wikipedia is a repository of the world's information. If you have an understanding of HTML, you can use Python to extract this information. In this video, you will: review the HTML of a basic web page; understand the Composition of an HTML Tag; understand HTML Trees; and understand HTML Tables. Let’s say you were asked to find the name and salary of players in a National Basketball League from the following web page. The web page is comprised of HTML. It consists of text surrounded by a series of blue text elements enclosed in angle brackets called tags. The tags tells the browser how to display the content. The data we require is in this text. 
The first portion contains the "DOCTYPE html” which declares this document is an HTML document. <html> element is the root element of an HTML page,  and <head> element contains meta information about the HTML page. Next, we have the body, this is what's displayed on the web page. This is usually the data we are interested in, we see the elements with an “h3”, this means type 3 heading, makes the text larger and bold. These tags have the names of the players, notice the data is enclosed in the elements. It starts with a h3 in brackets and ends in a slash h3 in brackets. There is also a different tag “p”, this means paragraph, each p tag contains a player's salary. Let’s take a closer look at the composition of an HTML tag. 
Here is an example of an HTML Anchor tag. It will display IBM and when you click it, it will send you to IBM.com. We have the tag name, in this case “a”. This tag defines a hyperlink, which is used to link from one page to another. It’s helpful to think of each tag name as a class in Python and each individual tag as an instance. We have the opening or start tag and we have the end tag. This has the tag name preceded by a slash. 
These tags contain the content, in this case what’s displayed on the web page. We have the attribute, this is composed of the Attribute Name and Attribute Value. In this case it the url to the destination web page. Real web pages are more complex, depending on your browser you can select the HTML element, then click Inspect. The result will give you the ability to inspect the HTML. There is also other types of content such as CSS and JavaScript that we will not go over in this course. The actual element is shown here. 
Each HTML document can actually be referred to as a document tree. Let's go over a simple example. Tags may contain strings and other tags. These elements are the tag’s children. We can represent this as a family tree. Each nested tag is a level in the tree. The tag HTML tag contains the head and body tag. 
The Head and body tag are the descendants of the html tag. In particular they are the children of the HTML tag. HTML tag is their parent. The head and body tag are siblings as they are on the same level. Title tag is the child of the head tag and its parent is the head tag. The title tag is a descendant of the HTML tag but not its child. The heading and paragraph tags are the children of the body tag; and as they are all children of the body tag they are siblings of each other. 
The bold tag is a child of the heading tag, the content of the tag is also part of the tree but this can get unwieldy to draw. Next, let’s review HTML tables. To define an HTML table we have the table tag. Each table row is defined with a  <tr>  tag, you can also use a table header tag for the first row. The table row cell contains a set of <td> tags, each defines a table cell. For the first row first cell we have; for the first row second cell we have; and so on. For the second row we have; and for the second row first cell we have; for the second row second cell we have; and so on. We now have some basic knowledge of HTML. 

In this video we will cover Webscraping. After watching this video you will be able to: define web scraping; understand the role of BeautifulSoup Objects; apply the find_all method; and webscrape a website. What would you do if you wanted to analyze hundreds of points of data to find the best players of a sports team? Would you start manually copying and pasting information from different websites into a spreadsheet? Spending hours trying to find the right data, and eventually giving up because the task was to overwhelming? That’s where webscraping can help. Webscraping is a process that can be used to automatically extract information from a website, and can easily be accomplished within a matter of minutes and not hours. To get started we just need a little Python code and the help of two modules named Requests and Beautiful Soup. 
Let’s say you were asked to find the name and salary of players in a National Basketball League, from the following webpage. We import BeautifulSoup. We can store the webpage HTML as a string in the variable HTML. To parse a document, pass it into the BeautifulSoup constructor. We get the Beautiful Soup object , soup, which represents the document as a nested data structure. BeautifulSoup represents HTML as a set of Tree like objects with methods used to parse the HTML. We will review the BeautifulSoup object Using the BeautifulSoup object, soup, we created The tag object corresponds to an HTML tag in the original document. For example, the tag “title.” 
Consider the tag <h3>. If there is more than one tag with the same name, the first element with that tag is selected. In this case with Lebron James, we see the name is Enclosed in the bold attribute "b". To extract it, use the Tree representation. Let’s use the Tree representation. The variable tag-object is located here. We can access the child of the tag or navigate down the branch as follows: You can navigate up the tree by using the parent attribute. 
The variable tag child is located here. We can access the parent. This is the original tag object. We can find the sibling of “tag object.” We simply use the next sibling attribute. We can find the sibling of sibling one. We simply use the next sibling attribute. 
Consider the tag child object. You can access the attribute name and value as a key value pair in a dictionary as follows. You can return the content as a Navigable string, this is like a Python string that supports BeautifulSoup functionality. Let's review the method find_all. This is a filter, you can use filters to filter based on a tag’s name, it’s attributes, the text of a string, or on some combination of these. Consider the list of pizza places. Like before, create a BeautifulSoup object. But this time, name it table. 
The find_all () method looks through a tag’s descendants and retrieves all descendants that match your filters. Apply it to the table with the tag <tr>. The result is a Python iterable just like a list, each element is a tag object for <tr>. This corresponds to each row in the list- including the table header. Each element is a tag object. Consider the first row. For example, we can extract the first table cell. 
We can also iterate through each table cell. First, we iterate through the list “table rows,” via the variable row. Each element corresponds to a row in the table. We can apply the method find all to find all the table cells, then we can iterate through the variable cells for each row. For each iteration, the variable cell corresponds to an element in the table for that particular row. We continue to iterate through each element and repeat the process for each row. Let’s see how to apply BeautifulSoup to a webpage. 
To scrape a webpage we also need the Requests library. The first step is to import the modules that are needed. Use the get method from the requests library to download the webpage. The input is the URL. Use the text attribute to get the text and assign it to the variable page. Then, create a BeautifulSoup object ‘soup’ from the variable page. It will allow you to parse through the HTML page. 

Reading the yfinance library:
import yfinance as yf
# Download historical data for a stock
msft = yf.Ticker("MSFT")
msft_data = msft.history(period="max")
# Display the downloaded data
msft_data.head()


Explanation for the above code:

First, import the yfinance library using the alias yf.
Then, create a Ticker object for the Microsoft stock (“MSFT”).
Use the history method of the Ticker object to download the historical data for the stock. The period parameter of the history method specifies when you want to download the data. In this example, it is set to max to download the maximum available historical data.
Here are some of the possible values for the period parameter and what they represent:

period="1d": Download 1 day of historical data.
period="5d": Download 5 days of historical data.
period="1mo": Download 1 month of historical data.
period="3mo": Download 3 months of historical data.
period="6mo": Download 6 months of historical data.
period="1y": Download 1 year of historical data.
period="2y": Download 2 years of historical data.
period="5y": Download 5 years of historical data.
period="10y": Download 10 years of historical data.
period="ytd": Download historical data since the beginning of the current year.
period="max": Download all available historical data.
Finally, you print the downloaded data using the head function. This downloaded data will display a Pandas data frame containing Microsoft's historical stock prices and other financial data.

Project Overview:
For this project, you will take on the role of a Data Scientist / Data Analyst working for a new startup investment firm that helps customers invest their money in stocks. Your task is to extract financial data, such as historical share prices and quarterly revenue reports, from various sources using Python libraries and web scraping techniques. After collecting this data, you will visualize it in a dashboard to identify patterns or trends. The stocks you will work with are Tesla, Amazon, AMD, and GameStop.

Dashboard Analytics Displayed
A dashboard provides a clear view of key insights. In this project, you will practice analyzing datasets and extracting insights to display on a dashboard. You will use prompts to effectively access and visualize data. The goal is to learn how to present insights clearly using Plotly for data visualization.

Submission Options
You have two options to submit your assignment:

Option 1: AI-Graded Submission and Evaluation  

Option 2: Peer-Graded Submission and Evaluation  
Review Criteria
This project includes two hands-on labs on extracting stock data and one final assignment. Your performance will be evaluated based on the completion of two quizzes and one graded assignment (AI or Peer Review). The quizzes will test your understanding of the lab outputs, and the assignment will require you to share screenshots and results from your completed work.
The final assignment integrates all the skills you have learned. In this exercise, you will apply them by using various Python functions and libraries to extract data, perform dataframe operations, and analyze results effectively.

How can you submit the deliverables
You can submit your project deliverables in one of the following ways:

Option 1: AI-Graded Submission and Evaluation

Complete all the exercises in the notebook.
After finishing, save and download your Jupyter Notebook file.
Submit the downloaded file for AI-based evaluation.
Once submitted, you will receive automated grades instantly.
Option 2: Peer-Graded Submission and Evaluation

Complete all exercises in the notebook.
Take screenshots of each completed exercise as instructed throughout this assignment.
Each screenshot should include your code and output (or markdown, as specified).
Screenshots can be saved in .png or .jpg format.
In this option, you will receive your grades based on the evaluation from other peers.
We recommend using Option 1 for faster grading. However, if you experience difficulties with this method or cannot access it, you may choose Option 2 instead.
Option 1: AI Graded - Final Project: Submission and Evaluation
 To submit your responses for the final project, select the “I agree to use this app responsibly” checkbox, and then click the "Launch App" button. This will open the final project submission tool in a new browser tab. 
This assignment submission uses "Mark", an AI grading tool from IBM Skills Network. Upon clicking "Launch App," your username and email will be passed to IBM Skills Network and will only be used for communicating important information to enhance your learning experience, in accordance with IBM Skills Network's privacy policy
This course uses a third-party app, Option 1: AI Graded - Final Project: Submission and Evaluation, to enhance your learning experience. The app will reference basic information like your name, email, and Coursera ID.

Peer-graded Assignment: Option 2: Peer Graded - Final Project Submission and Evaluation
DeadlineNov 17, 11:59 PM EST
Ready for the assignment?
You will find instructions below to submit.
At this point please ensure you have completed the two previous yfinance and web scraping labs. In this assignment you will upload screenshots of your code and results. You will also be reviewing the submission for one of your peers and grading their work.

As a data scientist working for an investment firm, you will extract the revenue data for Tesla and GameStop and build a dashboard to compare the price of the stock vs the revenue. 

Full Points: Working code that yields correct results

You will be graded on the dashboards displaying the specified data and the screenshots you took during the final project lab questions. There are 12 possible points for this assignment. Here is the breakdown:

Question 1 - Extracting Tesla Stock Data Using yfinance - 2 Points
Question 2 - Extracting Tesla Revenue Data Using Webscraping - 1 Points
Question 3 - Extracting GameStop Stock Data Using yfinance - 2 Points
Question 4 - Extracting GameStop Revenue Data Using Webscraping - 1 Points
Question 5 - Tesla Stock and Revenue Dashboard - 2 Points
Question 6 - GameStop Stock and Revenue Dashboard- 2 Points
Question 7 - Sharing your Assignment Notebook - 2 Points

For each problem points will be awarded as follows:

Full Points: Working code that yields correct results
Partial Points: Partially correct code or results
No Points: Did not attempt the problem or did not upload any solution


